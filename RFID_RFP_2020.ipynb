{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RFID RFP 2020",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2na5SCvgAsXQ"
      },
      "source": [
        "Notebook Description\r\n",
        "\r\n",
        "Please select an environment with a GPU available\r\n",
        "\r\n",
        "Open this folder with google drive: https://drive.google.com/drive/folders/1l7cvR1Z8V7eiVdqcPgv0i5rb2ZMSja63?usp=sharing\r\n",
        "\r\n",
        "Click on add shortcut to my drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku0uts1fAqNR",
        "outputId": "028db4f2-8cc9-4ebc-d3e0-797f367fc1d2"
      },
      "source": [
        "#Importing Dataframe\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "\r\n",
        "import os\r\n",
        "PATH_TRAIN = '/content/drive/My Drive/Public-RFID2020-RFP-Dataset/preprocessed/FILT-train/'\r\n",
        "PATH_VALID = '/content/drive/My Drive/Public-RFID2020-RFP-Dataset/preprocessed/FILT-validation/'\r\n",
        "PATH_TEST = \"/content/drive/My Drive/Public-RFID2020-RFP-Dataset/preprocessed/FILT-test/\"\r\n",
        "PATH_MODELS = \"/tmp/\"\r\n",
        "PATH_RESULTS = \"/tmp/\"\r\n",
        "os.path.isdir(PATH_TRAIN)    # False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQtk9BWnBxLI",
        "outputId": "ac4c2052-0e73-4e0a-95a7-74f1b861bc97"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "np.set_printoptions(suppress=True)\r\n",
        "np.random.seed(1)\r\n",
        "import torch.optim as optim\r\n",
        "from torchsummary import summary\r\n",
        "import copy\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RGT0iqeCcy0"
      },
      "source": [
        "#Parameters\r\n",
        "num_tags = 20\r\n",
        "perc_to_load = 0.1\r\n",
        "additive_id = 0\r\n",
        "batch_size = 64"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_02hS25pChmw"
      },
      "source": [
        "def load_data_split(root_path, start, end, first_id, last_id, letts,additive_id):\r\n",
        "    wave_piece_size = 1024#3072# 1024\r\n",
        "    X, y = [], []\r\n",
        "    for raw_signal_file in os.listdir(root_path):\r\n",
        "        \r\n",
        "        tag_id = raw_signal_file[5:8]\r\n",
        "        tag_let = raw_signal_file[8]\r\n",
        "        if (tag_let not in letts or first_id > int(tag_id) or last_id <= int(tag_id)):\r\n",
        "            continue\r\n",
        "        print(raw_signal_file)\r\n",
        "        signal_loaded = np.load(open(root_path + raw_signal_file, \"rb\"))\r\n",
        "        for wave_id in range(int(len(signal_loaded) * start), int(len(signal_loaded) * end)):\r\n",
        "            wave = signal_loaded[wave_id]\r\n",
        "            for i in range(0, len(wave) - wave_piece_size, 256):\r\n",
        "                x_tmp = np.asarray(wave[i:i + wave_piece_size])\r\n",
        "                X += [x_tmp]\r\n",
        "                y += [int(tag_id)-additive_id]\r\n",
        "    return X, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33je9D3UCmwo",
        "outputId": "b52012cd-2f17-4ee9-922e-e361ebb4684c"
      },
      "source": [
        "kernel_size = 3\r\n",
        "\r\n",
        "a0 = nn.Conv1d(in_channels= 2, out_channels= 25, kernel_size= 3, padding = (kernel_size // 2))\r\n",
        "a1 = nn.LeakyReLU(negative_slope=0.1)\r\n",
        "b0 = nn.MaxPool1d(kernel_size = 2, padding = 1)\r\n",
        "\r\n",
        "c0 = nn.Conv1d(in_channels= 25, out_channels= 25, kernel_size= 3, padding = (kernel_size // 2))\r\n",
        "c1 = nn.LeakyReLU(negative_slope=0.1)\r\n",
        "d0 = nn.MaxPool1d(2, padding = 1)\r\n",
        "\r\n",
        "e0 = nn.Flatten()   \r\n",
        "e1 = nn.Linear(6425,num_tags)\r\n",
        "\r\n",
        "model = nn.Sequential(a0,a1,b0,c0,c1,d0,e0,e1)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "def accuracy(predictions, labels):\r\n",
        "    classes = torch.argmax(predictions, dim=1)\r\n",
        "    return torch.mean((classes == labels).float())\r\n",
        "\r\n",
        "summary(model.cuda(),(2,1024))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1             [-1, 25, 1024]             175\n",
            "         LeakyReLU-2             [-1, 25, 1024]               0\n",
            "         MaxPool1d-3              [-1, 25, 513]               0\n",
            "            Conv1d-4              [-1, 25, 513]           1,900\n",
            "         LeakyReLU-5              [-1, 25, 513]               0\n",
            "         MaxPool1d-6              [-1, 25, 257]               0\n",
            "           Flatten-7                 [-1, 6425]               0\n",
            "            Linear-8                   [-1, 20]         128,520\n",
            "================================================================\n",
            "Total params: 130,595\n",
            "Trainable params: 130,595\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.78\n",
            "Params size (MB): 0.50\n",
            "Estimated Total Size (MB): 1.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AVQ7MdJCz4x"
      },
      "source": [
        "def client_update(client_model, optimizer, train_loader, epoch):\r\n",
        "    \"\"\"\r\n",
        "    This function updates/trains client model on client data\r\n",
        "    \"\"\"\r\n",
        "    client_model.train()\r\n",
        "    accs = []\r\n",
        "    for e in range(epoch):\r\n",
        "        running_accuracy = 0.\r\n",
        "        l = len((train_loader))\r\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "            #print(batch_idx, l)\r\n",
        "            #data, target = augment(data, target)\r\n",
        "            data, target = data.cuda(), target.cuda()\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            output = client_model(data)\r\n",
        "            loss = criterion(output, target.long())\r\n",
        "\r\n",
        "            #loss = F.nll_loss(output, target.long())\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            running_accuracy += accuracy(output, target)/len(train_loader)\r\n",
        "        print(str(running_accuracy.cpu().numpy()))\r\n",
        "        accs.append(running_accuracy.cpu().numpy())\r\n",
        "        #print(accs)\r\n",
        "\r\n",
        "    return loss.item(), accs\r\n",
        "\r\n",
        "def server_aggregate(global_model, client_models):\r\n",
        "    \"\"\"\r\n",
        "    This function has aggregation method 'mean'\r\n",
        "    \"\"\"\r\n",
        "    ### This will take simple mean of the weights of models ###\r\n",
        "    global_dict = global_model.state_dict()\r\n",
        "    for k in global_dict.keys():\r\n",
        "      global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\r\n",
        "    global_model.load_state_dict(global_dict)\r\n",
        "    for m in client_models:\r\n",
        "      m.load_state_dict(global_model.state_dict())\r\n",
        "    \r\n",
        "def test(testing_model, testing_loader):\r\n",
        "    \r\n",
        "    \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\r\n",
        "    testing_model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in testing_loader:\r\n",
        "            data, target = data.cuda(), target.cuda()#data, target#data.cuda(), target.cuda()\r\n",
        "            output = testing_model(data)\r\n",
        "            test_loss += F.nll_loss(output, target.long(), reduction='sum').item()  # sum up batch loss\r\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\r\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "    test_loss /= len(testing_loader.dataset)\r\n",
        "    acc = correct / len(testing_loader.dataset)\r\n",
        "    del testing_model\r\n",
        "\r\n",
        "    return test_loss, acc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOk1_9BEOgd"
      },
      "source": [
        "def augment(data, target):\r\n",
        "    pitch = [5,10,20,100]\r\n",
        "    data_ret, target_ret = data.clone(), target.clone()\r\n",
        "    means = data.mean(axis = 2)\r\n",
        "    for p in pitch:\r\n",
        "        gc.collect()\r\n",
        "        dr,tr = data.clone(), target.clone()\r\n",
        "        i = -1\r\n",
        "        for tensor in dr:\r\n",
        "            i += 1\r\n",
        "            tensor[:,0] = tensor[:,0].add(torch.normal(0, abs(means[i][0]/p), size=(1, data.shape[1])))\r\n",
        "            tensor[:,1] = tensor[:,1].add(torch.normal(0, abs(means[i][1]/p), size=(1, data.shape[1])))\r\n",
        "        data_ret = torch.cat((data_ret, dr), dim = 0)\r\n",
        "        target_ret = torch.cat((target_ret, tr), dim = 0)\r\n",
        "    return data_ret, target_ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjpTCCKZDL-3"
      },
      "source": [
        "# Parameters\r\n",
        "params = {'batch_size': batch_size,\r\n",
        "          'shuffle': True,\r\n",
        "          'num_workers': 4}\r\n",
        "\r\n",
        "def generaTrainSet(lettera):\r\n",
        "    X, y = load_data_split(PATH_TRAIN, 0, perc_to_load, additive_id, additive_id+num_tags, lettera,0)\r\n",
        "    X_train = torch.from_numpy(np.asarray(X)).float()\r\n",
        "    X_train = X_train.transpose(1, 2).contiguous()\r\n",
        "    y_train = torch.from_numpy(np.asarray(y))\r\n",
        "    #X_train, y_train = augment(X_train, y_train)\r\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\r\n",
        "    training_generator = torch.utils.data.DataLoader(dataset, **params)\r\n",
        "    return training_generator\r\n",
        "\r\n",
        "train_loader = [generaTrainSet(\"A\"),generaTrainSet(\"B\"),generaTrainSet(\"C\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWWNk7z7DWc0"
      },
      "source": [
        "Xt, yt = load_data_split(PATH_VALID, 0, 1, additive_id, additive_id+num_tags, \"ABC\",0)\r\n",
        "\r\n",
        "\r\n",
        "X_test = torch.from_numpy(np.asarray(Xt)).float()\r\n",
        "X_test = X_test.transpose(1, 2).contiguous()\r\n",
        "y_test = torch.from_numpy(np.asarray(yt))\r\n",
        "dataset_t = torch.utils.data.TensorDataset(X_test, y_test)\r\n",
        "\r\n",
        "\r\n",
        "# Parameters\r\n",
        "params = {'batch_size': batch_size,\r\n",
        "          'shuffle': True}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "test_generator = torch.utils.data.DataLoader(dataset_t, **params)\r\n",
        "test_loader = test_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_-6DyQxDqpP"
      },
      "source": [
        "num_clients = 3\r\n",
        "num_selected =3\r\n",
        "num_rounds = 100\r\n",
        "epochs = 1\r\n",
        "\r\n",
        "global_model =  copy.deepcopy(model).cuda()\r\n",
        "\r\n",
        "client_models = [ copy.deepcopy(model).cuda() for _ in range(num_selected)]\r\n",
        "for model in client_models:\r\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \r\n",
        "\r\n",
        "opt = [optim.Adam(model.parameters()) for model in client_models]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toar2uArDyal",
        "outputId": "b1ae6650-c29d-4292-eb13-8f2503368cbc"
      },
      "source": [
        "losses_train = []\r\n",
        "losses_test = []\r\n",
        "acc_train = []\r\n",
        "acc_test = []\r\n",
        "\r\n",
        "accs_tot = [[],[],[]]\r\n",
        "\r\n",
        "for r in range(num_rounds):\r\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\r\n",
        "    loss = 0\r\n",
        "    for i in range(num_selected):\r\n",
        "        print(\"CLI\",client_idx[i])\r\n",
        "        l, accs = client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\r\n",
        "        loss += l\r\n",
        "        for a in accs:\r\n",
        "            accs_tot[client_idx[i]].append(a)\r\n",
        " \r\n",
        "    losses_train.append(loss)\r\n",
        "    server_aggregate(global_model, client_models)\r\n",
        "    test_loss, acc = test(global_model, test_loader)\r\n",
        "    losses_test.append(test_loss)\r\n",
        "    acc_test.append(acc)\r\n",
        "    print('%d-th round' % r)\r\n",
        "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\r\n",
        "\r\n",
        "#print(accs_tot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLI 1\n",
            "0.7447815\n",
            "CLI 2\n",
            "0.7158822\n",
            "CLI 0\n",
            "0.7892004\n",
            "0-th round\n",
            "average train loss 250 | test loss -8.38e+03 | test acc: 0.236\n",
            "CLI 0\n",
            "0.83815414\n",
            "CLI 2\n",
            "0.73736066\n",
            "CLI 1\n",
            "0.7643393\n",
            "1-th round\n",
            "average train loss 79.4 | test loss -6.29e+03 | test acc: 0.203\n",
            "CLI 0\n",
            "0.82379705\n",
            "CLI 2\n",
            "0.72367615\n",
            "CLI 1\n",
            "0.80941004\n",
            "2-th round\n",
            "average train loss 580 | test loss -4.11e+03 | test acc: 0.225\n",
            "CLI 1\n",
            "0.8009314\n",
            "CLI 2\n",
            "0.7513725\n",
            "CLI 0\n",
            "0.79441524\n",
            "3-th round\n",
            "average train loss 154 | test loss -3.96e+03 | test acc: 0.200\n",
            "CLI 2\n",
            "0.7667065\n",
            "CLI 0\n",
            "0.8293879\n",
            "CLI 1\n",
            "0.8251801\n",
            "4-th round\n",
            "average train loss 90 | test loss -5.63e+03 | test acc: 0.295\n",
            "CLI 2\n",
            "0.7639945\n",
            "CLI 0\n",
            "0.8812146\n",
            "CLI 1\n",
            "0.86030847\n",
            "5-th round\n",
            "average train loss 131 | test loss -3.66e+03 | test acc: 0.330\n",
            "CLI 0\n",
            "0.8792429\n",
            "CLI 1\n",
            "0.8113218\n",
            "CLI 2\n",
            "0.78813684\n",
            "6-th round\n",
            "average train loss 160 | test loss -5.07e+03 | test acc: 0.282\n",
            "CLI 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdjLHjrEf8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}